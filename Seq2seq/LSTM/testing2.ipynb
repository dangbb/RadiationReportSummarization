{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "greater-joining",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data_path = \"E:\\\\MachineLearning\\\\Study\\\\RadiationReportSummarization\\\\Dataset\\\\train.csv\"\n",
    "train_path = \"E:\\\\MachineLearning\\\\Study\\\\RadiationReportSummarization\\\\Dataset\\\\train_set.csv\"\n",
    "valid_path = \"E:\\\\MachineLearning\\\\Study\\\\RadiationReportSummarization\\\\Dataset\\\\test_set.csv\"\n",
    "metadata_path = \"E:\\\\MachineLearning\\\\Study\\\\RadiationReportSummarization\\\\Dataset\\\\char_level.txt\"\n",
    "\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 100  # Number of samples to train on.\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, metadata_path):\n",
    "        self.path = metadata_path\n",
    "        self.load()\n",
    "\n",
    "    def load(self):\n",
    "        with open(self.path) as file:\n",
    "            data = json.load(file)\n",
    "            self.metadata = data['metadata'][0]\n",
    "            self.input_token_dict = dict([(x, i) for i, x in enumerate(self.metadata['input_char'])])\n",
    "            self.output_token_dict = dict([(x, i) for i, x in enumerate(self.metadata['target_char'])])\n",
    "\n",
    "    def get_index(self, char):\n",
    "        return self.input_token_dict[char]\n",
    "\n",
    "    def get_in_char_set(self):\n",
    "        return self.metadata['input_char']\n",
    "\n",
    "    def get_ta_char_set(self):\n",
    "        return self.metadata['target_char']\n",
    "\n",
    "    def get_num_en_token(self):\n",
    "        return self.metadata['num_en_token']\n",
    "\n",
    "    def get_num_de_token(self):\n",
    "        return self.metadata['num_de_token']\n",
    "\n",
    "    def get_max_en_len(self):\n",
    "        return self.metadata['max_en_seq_len']\n",
    "\n",
    "    def get_max_de_len(self):\n",
    "        return self.metadata['max_de_seq_len']\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, train_path, valid_path, lang: Lang):\n",
    "        self.train_path = train_path\n",
    "        self.valid_path = valid_path\n",
    "        self.lang = lang\n",
    "\n",
    "    def embedding(self, input):\n",
    "        encoder_input_data = np.zeros((len(input[0]), self.lang.get_max_en_len(), len(self.lang.get_in_char_set())), dtype='float32')\n",
    "        decoder_output_data = np.zeros((len(input[0]), self.lang.get_max_de_len(), len(self.lang.get_ta_char_set())), dtype='float32')\n",
    "        decoder_target_data = np.zeros((len(input[0]), self.lang.get_max_de_len(), len(self.lang.get_ta_char_set())), dtype='float32')\n",
    "\n",
    "        for i, (input_text, target_text) in enumerate(zip(input[0], input[1])):\n",
    "            for t, char in enumerate(input_text):\n",
    "                encoder_input_data[i, t, self.lang.get_index(char)] = 1\n",
    "            for t, char in enumerate(target_text):\n",
    "                decoder_output_data[i, t, self.lang.get_index(char)] = 1\n",
    "                if t > 0:\n",
    "                    decoder_target_data[i, t - 1, self.lang.get_index(char)] = 1\n",
    "\n",
    "        return ([encoder_input_data, decoder_output_data], decoder_target_data)\n",
    "\n",
    "    def load_data(self, is_train=True):\n",
    "        path = self.train_path if is_train else self.valid_path\n",
    "        df = pd.read_csv(path, index_col=0, header=None)\n",
    "        inputs = [x for x in df[3]]\n",
    "        outputs = ['\\t' + x + '\\n' for x in df[4]]\n",
    "        data = (inputs, outputs)\n",
    "        return self.embedding(data)\n",
    "\n",
    "    def generate_data(self, batch_size, step, is_train=True):\n",
    "        path = self.train_path if is_train else self.valid_path\n",
    "        idx = 1\n",
    "\n",
    "        while True:\n",
    "            df = pd.read_csv(path, skiprows=(idx - 1) * batch_size, nrows=batch_size, header=None, index_col=0)\n",
    "            inputs = [x for x in df[3]]\n",
    "            outputs = []\n",
    "            for x in df[4]:\n",
    "                outputs.append('\\t' + x + '\\n')\n",
    "            data = (inputs, outputs)\n",
    "            yield self.embedding(data)\n",
    "\n",
    "            if idx < step:\n",
    "                idx = idx + 1\n",
    "            else:\n",
    "                idx = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "comparative-shelf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 87)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None, 87)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 256), (None, 352256      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  352256      input_2[0][0]                    \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 87)     22359       lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 726,871\n",
      "Trainable params: 726,871\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "[<tf.Tensor 'lstm/PartitionedCall:0' shape=(None, 256) dtype=float32>, <tf.Tensor 'lstm/PartitionedCall:2' shape=(None, 256) dtype=float32>, <tf.Tensor 'lstm/PartitionedCall:3' shape=(None, 256) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data_path = \"E:\\\\MachineLearning\\\\Study\\\\RadiationReportSummarization\\\\Dataset\\\\train.csv\"\n",
    "train_path = \"E:\\\\MachineLearning\\\\Study\\\\RadiationReportSummarization\\\\Dataset\\\\train_set.csv\"\n",
    "valid_path = \"E:\\\\MachineLearning\\\\Study\\\\RadiationReportSummarization\\\\Dataset\\\\test_set.csv\"\n",
    "metadata_path = \"E:\\\\MachineLearning\\\\Study\\\\RadiationReportSummarization\\\\Dataset\\\\char_level.txt\"\n",
    "\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 100  # Number of samples to train on.\n",
    "\n",
    "lang = Lang(metadata_path)\n",
    "dataloader = DataLoader(train_path, valid_path, lang)\n",
    "\n",
    "num_encoder_tokens = lang.get_num_en_token()\n",
    "num_decoder_tokens = lang.get_num_de_token()\n",
    "input_characters = lang.get_in_char_set()\n",
    "target_characters = lang.get_ta_char_set()\n",
    "max_encoder_seq_length = lang.get_max_en_len()\n",
    "max_decoder_seq_length = lang.get_max_de_len()\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary()\n",
    "# print(model.layers[-1].input)\n",
    "print(model.layers[-3].output)\n",
    "\n",
    "step = int(np.ceil(73236 / batch_size))\n",
    "step_valid = int(np.ceil(18308 / batch_size))\n",
    "\n",
    "# Run training\n",
    "from keras.optimizers import *\n",
    "model.compile(optimizer=Adam(lr=0.01, beta_1=0.9, beta_2=0.999, decay=0.001), loss='categorical_crossentropy')\n",
    "\"\"\"model.fit(x=dataloader.generate_data(batch_size, step),\n",
    "          epochs=epochs,\n",
    "          validation_data=dataloader.generate_data(batch_size, step_valid))\"\"\"\n",
    "\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in lang.input_token_dict.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in lang.output_token_dict.items())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "communist-pepper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decode sentence:  Again seen is a opacity in the left mid lung which is mildly improved in appearance from the prior study. There is a consolidation at the base of the right lung which appears worse from the prior study. The cardiomediastinal silhouette and hilar contours are normal. There is no evidence of pneumothorax and there may be a small right pleural effusion.\n"
     ]
    }
   ],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, lang.get_index('\\t')] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "def decode(input):\n",
    "    result = \"\"\n",
    "    \n",
    "    for i in range(len(input[0])):\n",
    "        for j in range(len(input[0][i])):\n",
    "            if input[0][i][j] == 1:\n",
    "                result = result + reverse_input_char_index[j]\n",
    "            \n",
    "    return result\n",
    "\n",
    "input_seq = dataloader.generate_data(1, 1, False)\n",
    "\n",
    "for (input, output), target in input_seq:\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    \n",
    "    print(\"Decode sentence: \", decode(input))\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
